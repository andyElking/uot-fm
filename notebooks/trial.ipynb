{
 "cells": [
  {
   "cell_type": "code",
   "id": "8f6404b2423c3747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:02:13.403633Z",
     "start_time": "2024-08-29T14:02:13.094054Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# navigate one directory up\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Imports\n",
    "import blobfile as bf\n",
    "import logging\n",
    "from ml_collections import ConfigDict, config_dict\n",
    "from evaluate import evaluate\n",
    "from train import train\n",
    "from notebooks.utils_nb import config_from_path\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./api_key.env\")\n",
    "\n",
    "config_path = './configs/celeba64/female/fm.py'\n",
    "config = config_from_path(config_path)\n",
    "config.wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "assert config.wandb_key is not None\n",
    "wandb.login(key=config.wandb_key)\n",
    "print(config)\n",
    "training = config.training\n",
    "training.ckpt_freq = 1000\n",
    "training.eval_freq = 2000\n",
    "training.print_freq = 100\n",
    "training.num_steps = 10000\n",
    "\n",
    "workdir = \"../runs\"\n",
    "bf.makedirs(f\"{workdir}/logs\")\n",
    "logger = logging.getLogger()\n",
    "file_stream = open(f\"{workdir}/logs/{config.name}.txt\", \"w\")\n",
    "handler = logging.StreamHandler(file_stream)\n",
    "formatter = logging.Formatter(\"%(levelname)s - %(filename)s - %(asctime)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(\"INFO\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/andy/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  attribute_id: 20\n",
      "  centered: true\n",
      "  eval_labels:\n",
      "  - 15\n",
      "  - 17\n",
      "  - 35\n",
      "  eval_paired: true\n",
      "  map_forward: false\n",
      "  precomputed_stats_file: celeba64_female\n",
      "  random_crop: false\n",
      "  shape:\n",
      "  - 3\n",
      "  - 64\n",
      "  - 64\n",
      "  shuffle_buffer: 10000\n",
      "  source: celeba_attribute\n",
      "  target: celeba_attribute\n",
      "dt0: 0.0\n",
      "eval:\n",
      "  checkpoint_metric: fid\n",
      "  checkpoint_step: 0\n",
      "  compute_metrics: true\n",
      "  enable_fid: true\n",
      "  enable_mse: false\n",
      "  enable_path_lengths: true\n",
      "  labelwise: true\n",
      "  num_save_samples: 7\n",
      "  save_samples: true\n",
      "model:\n",
      "  attention_resolution:\n",
      "  - 32\n",
      "  - 16\n",
      "  - 8\n",
      "  biggan_sample: false\n",
      "  dim_head: 64\n",
      "  dim_mults:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  dropout: 0.1\n",
      "  heads: 4\n",
      "  hidden_size: 192\n",
      "  input_shape:\n",
      "  - 3\n",
      "  - 64\n",
      "  - 64\n",
      "  num_res_blocks: 3\n",
      "  type: unet\n",
      "  use_vae: false\n",
      "name: fm_celeba64_female\n",
      "optim:\n",
      "  beta_one: 0.9\n",
      "  beta_two: 0.999\n",
      "  ema_decay: 0.9999\n",
      "  eps: 1.0e-08\n",
      "  grad_clip: 1.0\n",
      "  learning_rate: 0.0001\n",
      "  optimizer: adam\n",
      "  schedule: constant\n",
      "  warmup: 0.0\n",
      "  weight_decay: 0\n",
      "overfit_to_one_batch: false\n",
      "seed: 42\n",
      "solver: tsit5\n",
      "t0: 0.0\n",
      "t1: 1.0\n",
      "task: translation\n",
      "training:\n",
      "  batch_size: 256\n",
      "  ckpt_freq: 10000\n",
      "  epsilon: 0.0\n",
      "  eval_freq: 50000\n",
      "  flow_sigma: 0.01\n",
      "  gamma: constant\n",
      "  matching: false\n",
      "  method: flow\n",
      "  num_steps: 400000\n",
      "  preemption_ckpt: false\n",
      "  print_freq: 1000\n",
      "  resume_ckpt: false\n",
      "  save_checkpoints: true\n",
      "  tau_a: 0.0\n",
      "  tau_b: 0.0\n",
      "wandb_entity: ''\n",
      "wandb_group: ''\n",
      "wandb_key: a3e7e9a6f38a66ce561112d21ae6b5a9ef707c07\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T13:57:14.883862Z",
     "start_time": "2024-08-29T13:57:14.356746Z"
    }
   },
   "cell_type": "code",
   "source": "train(config, workdir)\n",
   "id": "43640e2be678f917",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/celeba/list_attr_celeba.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train(config, workdir)\n",
      "File \u001B[0;32m/media/andy/5599B2AC14208F3D/PycharmProjects/uot-fm/train.py:47\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(config, workdir)\u001B[0m\n\u001B[1;32m     44\u001B[0m     vae_encode_fn, vae_decode_fn \u001B[38;5;241m=\u001B[39m get_vae_fns(shard)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranslation\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 47\u001B[0m     train_src_ds, train_tgt_ds, eval_src_ds, eval_tgt_ds \u001B[38;5;241m=\u001B[39m get_translation_datasets(\n\u001B[1;32m     48\u001B[0m         config, shard, vae_encode_fn \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39muse_vae \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     49\u001B[0m     )\n\u001B[1;32m     50\u001B[0m     train_src_loader, train_tgt_loader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(train_src_ds), \u001B[38;5;28miter\u001B[39m(train_tgt_ds)\n\u001B[1;32m     51\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_train_src: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_src_ds\u001B[38;5;241m.\u001B[39mlength\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/media/andy/5599B2AC14208F3D/PycharmProjects/uot-fm/utils/datasets.py:24\u001B[0m, in \u001B[0;36mget_translation_datasets\u001B[0;34m(config, shard, vae_encode_fn)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_translation_datasets\u001B[39m(\n\u001B[1;32m     19\u001B[0m     config: ConfigDict,\n\u001B[1;32m     20\u001B[0m     shard: Optional[jax\u001B[38;5;241m.\u001B[39msharding\u001B[38;5;241m.\u001B[39mSharding] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     21\u001B[0m     vae_encode_fn: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     22\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset]:\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get translation datasets and prepare them.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     train_source_data, train_target_data, eval_source_data, eval_target_data \u001B[38;5;241m=\u001B[39m get_data(config, shard, vae_encode_fn)\n\u001B[1;32m     25\u001B[0m     train_source_ds \u001B[38;5;241m=\u001B[39m prepare_dataset(train_source_data, config)\n\u001B[1;32m     26\u001B[0m     eval_source_ds \u001B[38;5;241m=\u001B[39m prepare_dataset(eval_source_data, config, evaluation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/media/andy/5599B2AC14208F3D/PycharmProjects/uot-fm/utils/datasets.py:111\u001B[0m, in \u001B[0;36mget_data\u001B[0;34m(config, shard, vae_encode_fn)\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    105\u001B[0m         preprocess_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     (\n\u001B[1;32m    107\u001B[0m         train_source_data,\n\u001B[1;32m    108\u001B[0m         train_target_data,\n\u001B[1;32m    109\u001B[0m         train_source_label,\n\u001B[1;32m    110\u001B[0m         train_target_label,\n\u001B[0;32m--> 111\u001B[0m     ) \u001B[38;5;241m=\u001B[39m celeba_attribute(\n\u001B[1;32m    112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    113\u001B[0m         config\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mattribute_id,\n\u001B[1;32m    114\u001B[0m         config\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mmap_forward,\n\u001B[1;32m    115\u001B[0m         config\u001B[38;5;241m.\u001B[39mtraining\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m    116\u001B[0m         config\u001B[38;5;241m.\u001B[39moverfit_to_one_batch,\n\u001B[1;32m    117\u001B[0m         shard,\n\u001B[1;32m    118\u001B[0m         vae_encode_fn,\n\u001B[1;32m    119\u001B[0m         preprocess_fn,\n\u001B[1;32m    120\u001B[0m     )\n\u001B[1;32m    121\u001B[0m     (\n\u001B[1;32m    122\u001B[0m         eval_source_data,\n\u001B[1;32m    123\u001B[0m         eval_target_data,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    134\u001B[0m         preprocess_fn,\n\u001B[1;32m    135\u001B[0m     )\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mtarget \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgaussian\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m/media/andy/5599B2AC14208F3D/PycharmProjects/uot-fm/utils/datasets.py:227\u001B[0m, in \u001B[0;36mceleba_attribute\u001B[0;34m(split, attribute_id, map_forward, batch_size, overfit_to_one_batch, shard, vae_encode_fn, preprocess_fn, subset_attribute_id)\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;124;03mLoad celeba attribute data.\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    subset_attribute_id: Subset attribute id to split on (0-39)\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    226\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data/celeba\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 227\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/list_attr_celeba.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m csv_file:\n\u001B[1;32m    228\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(csv\u001B[38;5;241m.\u001B[39mreader(csv_file, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, skipinitialspace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    229\u001B[0m     data \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m2\u001B[39m:]\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/celeba/list_attr_celeba.txt'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f4c99e54a49a793a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
